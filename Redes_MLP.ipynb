{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOvaILh7AdirzqNEOfhieGr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joseevitor/My-curriculum/blob/main/Redes_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aluno: José Vítor Miranda\n",
        "Rede MLP"
      ],
      "metadata": {
        "id": "PcDmwQpOqU8p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import biblioteca MLP e primeiro comando"
      ],
      "metadata": {
        "id": "BiDP-Lfb6eYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neural_network import MLPClassifier"
      ],
      "metadata": {
        "id": "kXLPquaorQhc"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = MLPClassifier(hidden_layer_sizes=(3, ), activation = \"logistic\", verbose=True, learning_rate_init=0.08)"
      ],
      "metadata": {
        "id": "BehtSeaErb0z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9Uc5OdSP6Agg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# fazer uma funçãozinha pra abrir o braily"
      ],
      "metadata": {
        "id": "6P-QB-Nb6B4e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "3SpYKVPmrm6e"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def braille():\n",
        "    entrada = []\n",
        "    saida = []\n",
        "    for i in range(64):\n",
        "        symbol = [int(x) for x in f'{i:06b}']\n",
        "        entrada.append(symbol)\n",
        "        saida.append(i)\n",
        "\n",
        "    return np.array(entrada), np.array(saida)\n",
        "\n"
      ],
      "metadata": {
        "id": "_2H-3Mbm54r3"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chamando a função pra gerar os dados\n",
        "X, y = braille()"
      ],
      "metadata": {
        "id": "6lGUD4voFf32"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tenho que verificar o formato pq antes deu problema\n",
        "print(\"Shape do X: \", X.shape)\n",
        "print(\"Shape y: \", y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMpEvGQkFeE9",
        "outputId": "ea4a3082-1489-49aa-a80e-10b55a78a292"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape do X:  (64, 6)\n",
            "Shape y:  (64,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3YTd5lqqSrq",
        "outputId": "85ae71d7-cb7d-4a62-b88b-c93b81fd3723"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 1],\n",
              "        [0, 0, 0, 0, 1, 0],\n",
              "        [0, 0, 0, 0, 1, 1],\n",
              "        [0, 0, 0, 1, 0, 0],\n",
              "        [0, 0, 0, 1, 0, 1],\n",
              "        [0, 0, 0, 1, 1, 0],\n",
              "        [0, 0, 0, 1, 1, 1],\n",
              "        [0, 0, 1, 0, 0, 0],\n",
              "        [0, 0, 1, 0, 0, 1],\n",
              "        [0, 0, 1, 0, 1, 0],\n",
              "        [0, 0, 1, 0, 1, 1],\n",
              "        [0, 0, 1, 1, 0, 0],\n",
              "        [0, 0, 1, 1, 0, 1],\n",
              "        [0, 0, 1, 1, 1, 0],\n",
              "        [0, 0, 1, 1, 1, 1],\n",
              "        [0, 1, 0, 0, 0, 0],\n",
              "        [0, 1, 0, 0, 0, 1],\n",
              "        [0, 1, 0, 0, 1, 0],\n",
              "        [0, 1, 0, 0, 1, 1],\n",
              "        [0, 1, 0, 1, 0, 0],\n",
              "        [0, 1, 0, 1, 0, 1],\n",
              "        [0, 1, 0, 1, 1, 0],\n",
              "        [0, 1, 0, 1, 1, 1],\n",
              "        [0, 1, 1, 0, 0, 0],\n",
              "        [0, 1, 1, 0, 0, 1],\n",
              "        [0, 1, 1, 0, 1, 0],\n",
              "        [0, 1, 1, 0, 1, 1],\n",
              "        [0, 1, 1, 1, 0, 0],\n",
              "        [0, 1, 1, 1, 0, 1],\n",
              "        [0, 1, 1, 1, 1, 0],\n",
              "        [0, 1, 1, 1, 1, 1],\n",
              "        [1, 0, 0, 0, 0, 0],\n",
              "        [1, 0, 0, 0, 0, 1],\n",
              "        [1, 0, 0, 0, 1, 0],\n",
              "        [1, 0, 0, 0, 1, 1],\n",
              "        [1, 0, 0, 1, 0, 0],\n",
              "        [1, 0, 0, 1, 0, 1],\n",
              "        [1, 0, 0, 1, 1, 0],\n",
              "        [1, 0, 0, 1, 1, 1],\n",
              "        [1, 0, 1, 0, 0, 0],\n",
              "        [1, 0, 1, 0, 0, 1],\n",
              "        [1, 0, 1, 0, 1, 0],\n",
              "        [1, 0, 1, 0, 1, 1],\n",
              "        [1, 0, 1, 1, 0, 0],\n",
              "        [1, 0, 1, 1, 0, 1],\n",
              "        [1, 0, 1, 1, 1, 0],\n",
              "        [1, 0, 1, 1, 1, 1],\n",
              "        [1, 1, 0, 0, 0, 0],\n",
              "        [1, 1, 0, 0, 0, 1],\n",
              "        [1, 1, 0, 0, 1, 0],\n",
              "        [1, 1, 0, 0, 1, 1],\n",
              "        [1, 1, 0, 1, 0, 0],\n",
              "        [1, 1, 0, 1, 0, 1],\n",
              "        [1, 1, 0, 1, 1, 0],\n",
              "        [1, 1, 0, 1, 1, 1],\n",
              "        [1, 1, 1, 0, 0, 0],\n",
              "        [1, 1, 1, 0, 0, 1],\n",
              "        [1, 1, 1, 0, 1, 0],\n",
              "        [1, 1, 1, 0, 1, 1],\n",
              "        [1, 1, 1, 1, 0, 0],\n",
              "        [1, 1, 1, 1, 0, 1],\n",
              "        [1, 1, 1, 1, 1, 0],\n",
              "        [1, 1, 1, 1, 1, 1]]),\n",
              " array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
              "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
              "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
              "        51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63]))"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "X, y"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Para eu poder treinar, tenho que mudar o mlp pra 2d, senão dá erro!!"
      ],
      "metadata": {
        "id": "i5eCFm1a_yGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp.fit(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_s5TeVgH-tuQ",
        "outputId": "9ebef7ab-eb31-416c-de33-da1b849201a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 4.17003480\n",
            "Iteration 2, loss = 4.15196654\n",
            "Iteration 3, loss = 4.13974529\n",
            "Iteration 4, loss = 4.12323246\n",
            "Iteration 5, loss = 4.10067441\n",
            "Iteration 6, loss = 4.07268363\n",
            "Iteration 7, loss = 4.03999460\n",
            "Iteration 8, loss = 4.00327255\n",
            "Iteration 9, loss = 3.96325553\n",
            "Iteration 10, loss = 3.92071284\n",
            "Iteration 11, loss = 3.87606702\n",
            "Iteration 12, loss = 3.82940037\n",
            "Iteration 13, loss = 3.78091773\n",
            "Iteration 14, loss = 3.73092648\n",
            "Iteration 15, loss = 3.67969683\n",
            "Iteration 16, loss = 3.62755291\n",
            "Iteration 17, loss = 3.57484272\n",
            "Iteration 18, loss = 3.52176235\n",
            "Iteration 19, loss = 3.46844344\n",
            "Iteration 20, loss = 3.41516612\n",
            "Iteration 21, loss = 3.36220997\n",
            "Iteration 22, loss = 3.30965770\n",
            "Iteration 23, loss = 3.25753082\n",
            "Iteration 24, loss = 3.20593103\n",
            "Iteration 25, loss = 3.15499997\n",
            "Iteration 26, loss = 3.10487611\n",
            "Iteration 27, loss = 3.05564301\n",
            "Iteration 28, loss = 3.00730994\n",
            "Iteration 29, loss = 2.95992198\n",
            "Iteration 30, loss = 2.91359903\n",
            "Iteration 31, loss = 2.86843787\n",
            "Iteration 32, loss = 2.82450754\n",
            "Iteration 33, loss = 2.78189375\n",
            "Iteration 34, loss = 2.74066830\n",
            "Iteration 35, loss = 2.70087635\n",
            "Iteration 36, loss = 2.66254073\n",
            "Iteration 37, loss = 2.62565478\n",
            "Iteration 38, loss = 2.59022685\n",
            "Iteration 39, loss = 2.55628530\n",
            "Iteration 40, loss = 2.52382812\n",
            "Iteration 41, loss = 2.49283552\n",
            "Iteration 42, loss = 2.46327929\n",
            "Iteration 43, loss = 2.43511479\n",
            "Iteration 44, loss = 2.40830032\n",
            "Iteration 45, loss = 2.38278645\n",
            "Iteration 46, loss = 2.35851464\n",
            "Iteration 47, loss = 2.33542623\n",
            "Iteration 48, loss = 2.31344922\n",
            "Iteration 49, loss = 2.29251068\n",
            "Iteration 50, loss = 2.27253732\n",
            "Iteration 51, loss = 2.25344957\n",
            "Iteration 52, loss = 2.23517381\n",
            "Iteration 53, loss = 2.21763716\n",
            "Iteration 54, loss = 2.20076966\n",
            "Iteration 55, loss = 2.18450433\n",
            "Iteration 56, loss = 2.16877380\n",
            "Iteration 57, loss = 2.15352015\n",
            "Iteration 58, loss = 2.13869096\n",
            "Iteration 59, loss = 2.12423929\n",
            "Iteration 60, loss = 2.11012420\n",
            "Iteration 61, loss = 2.09630761\n",
            "Iteration 62, loss = 2.08275828\n",
            "Iteration 63, loss = 2.06944847\n",
            "Iteration 64, loss = 2.05635628\n",
            "Iteration 65, loss = 2.04346578\n",
            "Iteration 66, loss = 2.03076533\n",
            "Iteration 67, loss = 2.01824916\n",
            "Iteration 68, loss = 2.00591422\n",
            "Iteration 69, loss = 1.99376077\n",
            "Iteration 70, loss = 1.98178858\n",
            "Iteration 71, loss = 1.96999744\n",
            "Iteration 72, loss = 1.95838773\n",
            "Iteration 73, loss = 1.94696042\n",
            "Iteration 74, loss = 1.93571586\n",
            "Iteration 75, loss = 1.92465230\n",
            "Iteration 76, loss = 1.91376783\n",
            "Iteration 77, loss = 1.90305907\n",
            "Iteration 78, loss = 1.89252170\n",
            "Iteration 79, loss = 1.88215098\n",
            "Iteration 80, loss = 1.87194317\n",
            "Iteration 81, loss = 1.86189404\n",
            "Iteration 82, loss = 1.85199981\n",
            "Iteration 83, loss = 1.84225602\n",
            "Iteration 84, loss = 1.83265795\n",
            "Iteration 85, loss = 1.82320024\n",
            "Iteration 86, loss = 1.81387674\n",
            "Iteration 87, loss = 1.80468074\n",
            "Iteration 88, loss = 1.79560548\n",
            "Iteration 89, loss = 1.78664384\n",
            "Iteration 90, loss = 1.77778898\n",
            "Iteration 91, loss = 1.76903452\n",
            "Iteration 92, loss = 1.76037457\n",
            "Iteration 93, loss = 1.75180368\n",
            "Iteration 94, loss = 1.74331627\n",
            "Iteration 95, loss = 1.73490716\n",
            "Iteration 96, loss = 1.72657123\n",
            "Iteration 97, loss = 1.71830376\n",
            "Iteration 98, loss = 1.71010033\n",
            "Iteration 99, loss = 1.70195674\n",
            "Iteration 100, loss = 1.69386911\n",
            "Iteration 101, loss = 1.68583380\n",
            "Iteration 102, loss = 1.67784753\n",
            "Iteration 103, loss = 1.66990729\n",
            "Iteration 104, loss = 1.66201025\n",
            "Iteration 105, loss = 1.65415379\n",
            "Iteration 106, loss = 1.64633592\n",
            "Iteration 107, loss = 1.63855532\n",
            "Iteration 108, loss = 1.63081141\n",
            "Iteration 109, loss = 1.62310439\n",
            "Iteration 110, loss = 1.61543532\n",
            "Iteration 111, loss = 1.60780594\n",
            "Iteration 112, loss = 1.60021875\n",
            "Iteration 113, loss = 1.59267678\n",
            "Iteration 114, loss = 1.58518369\n",
            "Iteration 115, loss = 1.57774354\n",
            "Iteration 116, loss = 1.57036074\n",
            "Iteration 117, loss = 1.56303990\n",
            "Iteration 118, loss = 1.55578581\n",
            "Iteration 119, loss = 1.54860331\n",
            "Iteration 120, loss = 1.54149746\n",
            "Iteration 121, loss = 1.53447378\n",
            "Iteration 122, loss = 1.52754019\n",
            "Iteration 123, loss = 1.52071688\n",
            "Iteration 124, loss = 1.51408171\n",
            "Iteration 125, loss = 1.50779552\n",
            "Iteration 126, loss = 1.50129281\n",
            "Iteration 127, loss = 1.49433548\n",
            "Iteration 128, loss = 1.48834809\n",
            "Iteration 129, loss = 1.48200486\n",
            "Iteration 130, loss = 1.47575928\n",
            "Iteration 131, loss = 1.46999235\n",
            "Iteration 132, loss = 1.46382294\n",
            "Iteration 133, loss = 1.45827110\n",
            "Iteration 134, loss = 1.45242199\n",
            "Iteration 135, loss = 1.44693265\n",
            "Iteration 136, loss = 1.44143900\n",
            "Iteration 137, loss = 1.43603084\n",
            "Iteration 138, loss = 1.43081473\n",
            "Iteration 139, loss = 1.42555363\n",
            "Iteration 140, loss = 1.42053208\n",
            "Iteration 141, loss = 1.41545803\n",
            "Iteration 142, loss = 1.41058785\n",
            "Iteration 143, loss = 1.40569904\n",
            "Iteration 144, loss = 1.40096624\n",
            "Iteration 145, loss = 1.39624658\n",
            "Iteration 146, loss = 1.39164205\n",
            "Iteration 147, loss = 1.38708060\n",
            "Iteration 148, loss = 1.38258723\n",
            "Iteration 149, loss = 1.37817771\n",
            "Iteration 150, loss = 1.37378387\n",
            "Iteration 151, loss = 1.36950801\n",
            "Iteration 152, loss = 1.36522037\n",
            "Iteration 153, loss = 1.36104636\n",
            "Iteration 154, loss = 1.35688109\n",
            "Iteration 155, loss = 1.35278125\n",
            "Iteration 156, loss = 1.34873917\n",
            "Iteration 157, loss = 1.34471155\n",
            "Iteration 158, loss = 1.34076873\n",
            "Iteration 159, loss = 1.33683076\n",
            "Iteration 160, loss = 1.33295911\n",
            "Iteration 161, loss = 1.32911895\n",
            "Iteration 162, loss = 1.32531146\n",
            "Iteration 163, loss = 1.32155638\n",
            "Iteration 164, loss = 1.31782195\n",
            "Iteration 165, loss = 1.31413559\n",
            "Iteration 166, loss = 1.31047705\n",
            "Iteration 167, loss = 1.30685509\n",
            "Iteration 168, loss = 1.30326682\n",
            "Iteration 169, loss = 1.29970822\n",
            "Iteration 170, loss = 1.29618658\n",
            "Iteration 171, loss = 1.29268921\n",
            "Iteration 172, loss = 1.28922874\n",
            "Iteration 173, loss = 1.28579549\n",
            "Iteration 174, loss = 1.28238899\n",
            "Iteration 175, loss = 1.27901782\n",
            "Iteration 176, loss = 1.27566864\n",
            "Iteration 177, loss = 1.27234960\n",
            "Iteration 178, loss = 1.26906024\n",
            "Iteration 179, loss = 1.26579316\n",
            "Iteration 180, loss = 1.26255565\n",
            "Iteration 181, loss = 1.25934352\n",
            "Iteration 182, loss = 1.25615531\n",
            "Iteration 183, loss = 1.25299385\n",
            "Iteration 184, loss = 1.24985553\n",
            "Iteration 185, loss = 1.24674182\n",
            "Iteration 186, loss = 1.24365207\n",
            "Iteration 187, loss = 1.24058400\n",
            "Iteration 188, loss = 1.23754002\n",
            "Iteration 189, loss = 1.23451813\n",
            "Iteration 190, loss = 1.23151663\n",
            "Iteration 191, loss = 1.22853766\n",
            "Iteration 192, loss = 1.22557955\n",
            "Iteration 193, loss = 1.22264086\n",
            "Iteration 194, loss = 1.21972267\n",
            "Iteration 195, loss = 1.21682394\n",
            "Iteration 196, loss = 1.21394378\n",
            "Iteration 197, loss = 1.21108245\n",
            "Iteration 198, loss = 1.20823892\n",
            "Iteration 199, loss = 1.20541256\n",
            "Iteration 200, loss = 1.20260363\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='logistic', hidden_layer_sizes=(3,),\n",
              "              learning_rate_init=0.08, verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(3,),\n",
              "              learning_rate_init=0.08, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(3,),\n",
              "              learning_rate_init=0.08, verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treino/teste"
      ],
      "metadata": {
        "id": "BpjBRuPMB7Bj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "MJ4dT5JODBtN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# treinos e testes padrão\n",
        "X_treino, X_teste, y_treino, y_teste = train_test_split(X, y, test_size=0.3)"
      ],
      "metadata": {
        "id": "7qjTJ1wBB1U1"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# treinar o algoritmo\n",
        "mlp.fit(X_treino, y_treino)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RrWEzjKIsNSX",
        "outputId": "a43eac26-65d0-4d1e-a487-91f420c8b7c6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 3.79547469\n",
            "Iteration 2, loss = 3.77546695\n",
            "Iteration 3, loss = 3.76102983\n",
            "Iteration 4, loss = 3.74290919\n",
            "Iteration 5, loss = 3.71889223\n",
            "Iteration 6, loss = 3.68920798\n",
            "Iteration 7, loss = 3.65459551\n",
            "Iteration 8, loss = 3.61586818\n",
            "Iteration 9, loss = 3.57353857\n",
            "Iteration 10, loss = 3.52770786\n",
            "Iteration 11, loss = 3.47859308\n",
            "Iteration 12, loss = 3.42661190\n",
            "Iteration 13, loss = 3.37214543\n",
            "Iteration 14, loss = 3.31552962\n",
            "Iteration 15, loss = 3.25728652\n",
            "Iteration 16, loss = 3.19800523\n",
            "Iteration 17, loss = 3.13812444\n",
            "Iteration 18, loss = 3.07799436\n",
            "Iteration 19, loss = 3.01793740\n",
            "Iteration 20, loss = 2.95824439\n",
            "Iteration 21, loss = 2.89924739\n",
            "Iteration 22, loss = 2.84127124\n",
            "Iteration 23, loss = 2.78449951\n",
            "Iteration 24, loss = 2.72910431\n",
            "Iteration 25, loss = 2.67524910\n",
            "Iteration 26, loss = 2.62300218\n",
            "Iteration 27, loss = 2.57245496\n",
            "Iteration 28, loss = 2.52367483\n",
            "Iteration 29, loss = 2.47671033\n",
            "Iteration 30, loss = 2.43162885\n",
            "Iteration 31, loss = 2.38843271\n",
            "Iteration 32, loss = 2.34711548\n",
            "Iteration 33, loss = 2.30767391\n",
            "Iteration 34, loss = 2.27007470\n",
            "Iteration 35, loss = 2.23428256\n",
            "Iteration 36, loss = 2.20023329\n",
            "Iteration 37, loss = 2.16784833\n",
            "Iteration 38, loss = 2.13706883\n",
            "Iteration 39, loss = 2.10781703\n",
            "Iteration 40, loss = 2.07999746\n",
            "Iteration 41, loss = 2.05352141\n",
            "Iteration 42, loss = 2.02830670\n",
            "Iteration 43, loss = 2.00427940\n",
            "Iteration 44, loss = 1.98136638\n",
            "Iteration 45, loss = 1.95949231\n",
            "Iteration 46, loss = 1.93859030\n",
            "Iteration 47, loss = 1.91859784\n",
            "Iteration 48, loss = 1.89944745\n",
            "Iteration 49, loss = 1.88107451\n",
            "Iteration 50, loss = 1.86342255\n",
            "Iteration 51, loss = 1.84643349\n",
            "Iteration 52, loss = 1.83004843\n",
            "Iteration 53, loss = 1.81422058\n",
            "Iteration 54, loss = 1.79891217\n",
            "Iteration 55, loss = 1.78408390\n",
            "Iteration 56, loss = 1.76970031\n",
            "Iteration 57, loss = 1.75573511\n",
            "Iteration 58, loss = 1.74216333\n",
            "Iteration 59, loss = 1.72895888\n",
            "Iteration 60, loss = 1.71610219\n",
            "Iteration 61, loss = 1.70358121\n",
            "Iteration 62, loss = 1.69138392\n",
            "Iteration 63, loss = 1.67949641\n",
            "Iteration 64, loss = 1.66790805\n",
            "Iteration 65, loss = 1.65660945\n",
            "Iteration 66, loss = 1.64558900\n",
            "Iteration 67, loss = 1.63483655\n",
            "Iteration 68, loss = 1.62434186\n",
            "Iteration 69, loss = 1.61409283\n",
            "Iteration 70, loss = 1.60407788\n",
            "Iteration 71, loss = 1.59428618\n",
            "Iteration 72, loss = 1.58470664\n",
            "Iteration 73, loss = 1.57532843\n",
            "Iteration 74, loss = 1.56614205\n",
            "Iteration 75, loss = 1.55713640\n",
            "Iteration 76, loss = 1.54830035\n",
            "Iteration 77, loss = 1.53962403\n",
            "Iteration 78, loss = 1.53109739\n",
            "Iteration 79, loss = 1.52271021\n",
            "Iteration 80, loss = 1.51445390\n",
            "Iteration 81, loss = 1.50632032\n",
            "Iteration 82, loss = 1.49830142\n",
            "Iteration 83, loss = 1.49039000\n",
            "Iteration 84, loss = 1.48257909\n",
            "Iteration 85, loss = 1.47486238\n",
            "Iteration 86, loss = 1.46723474\n",
            "Iteration 87, loss = 1.45969110\n",
            "Iteration 88, loss = 1.45222692\n",
            "Iteration 89, loss = 1.44483873\n",
            "Iteration 90, loss = 1.43752346\n",
            "Iteration 91, loss = 1.43027852\n",
            "Iteration 92, loss = 1.42310189\n",
            "Iteration 93, loss = 1.41599146\n",
            "Iteration 94, loss = 1.40894556\n",
            "Iteration 95, loss = 1.40196259\n",
            "Iteration 96, loss = 1.39504096\n",
            "Iteration 97, loss = 1.38817930\n",
            "Iteration 98, loss = 1.38137606\n",
            "Iteration 99, loss = 1.37462970\n",
            "Iteration 100, loss = 1.36793871\n",
            "Iteration 101, loss = 1.36130139\n",
            "Iteration 102, loss = 1.35471600\n",
            "Iteration 103, loss = 1.34818063\n",
            "Iteration 104, loss = 1.34169333\n",
            "Iteration 105, loss = 1.33525207\n",
            "Iteration 106, loss = 1.32885459\n",
            "Iteration 107, loss = 1.32249868\n",
            "Iteration 108, loss = 1.31618199\n",
            "Iteration 109, loss = 1.30990216\n",
            "Iteration 110, loss = 1.30365670\n",
            "Iteration 111, loss = 1.29744311\n",
            "Iteration 112, loss = 1.29125891\n",
            "Iteration 113, loss = 1.28510155\n",
            "Iteration 114, loss = 1.27896854\n",
            "Iteration 115, loss = 1.27285735\n",
            "Iteration 116, loss = 1.26676552\n",
            "Iteration 117, loss = 1.26069062\n",
            "Iteration 118, loss = 1.25463027\n",
            "Iteration 119, loss = 1.24858213\n",
            "Iteration 120, loss = 1.24254397\n",
            "Iteration 121, loss = 1.23651366\n",
            "Iteration 122, loss = 1.23048915\n",
            "Iteration 123, loss = 1.22446850\n",
            "Iteration 124, loss = 1.21844988\n",
            "Iteration 125, loss = 1.21243168\n",
            "Iteration 126, loss = 1.20641241\n",
            "Iteration 127, loss = 1.20039080\n",
            "Iteration 128, loss = 1.19436584\n",
            "Iteration 129, loss = 1.18833675\n",
            "Iteration 130, loss = 1.18230304\n",
            "Iteration 131, loss = 1.17626451\n",
            "Iteration 132, loss = 1.17022126\n",
            "Iteration 133, loss = 1.16417376\n",
            "Iteration 134, loss = 1.15812281\n",
            "Iteration 135, loss = 1.15206957\n",
            "Iteration 136, loss = 1.14601559\n",
            "Iteration 137, loss = 1.13996275\n",
            "Iteration 138, loss = 1.13391330\n",
            "Iteration 139, loss = 1.12786984\n",
            "Iteration 140, loss = 1.12183526\n",
            "Iteration 141, loss = 1.11581277\n",
            "Iteration 142, loss = 1.10980587\n",
            "Iteration 143, loss = 1.10381825\n",
            "Iteration 144, loss = 1.09785385\n",
            "Iteration 145, loss = 1.09191679\n",
            "Iteration 146, loss = 1.08601138\n",
            "Iteration 147, loss = 1.08014235\n",
            "Iteration 148, loss = 1.07431580\n",
            "Iteration 149, loss = 1.06854446\n",
            "Iteration 150, loss = 1.06286004\n",
            "Iteration 151, loss = 1.05735197\n",
            "Iteration 152, loss = 1.05180696\n",
            "Iteration 153, loss = 1.04601935\n",
            "Iteration 154, loss = 1.04046635\n",
            "Iteration 155, loss = 1.03520800\n",
            "Iteration 156, loss = 1.02969881\n",
            "Iteration 157, loss = 1.02439756\n",
            "Iteration 158, loss = 1.01928699\n",
            "Iteration 159, loss = 1.01401687\n",
            "Iteration 160, loss = 1.00901717\n",
            "Iteration 161, loss = 1.00404903\n",
            "Iteration 162, loss = 0.99907198\n",
            "Iteration 163, loss = 0.99432661\n",
            "Iteration 164, loss = 0.98954165\n",
            "Iteration 165, loss = 0.98487247\n",
            "Iteration 166, loss = 0.98033158\n",
            "Iteration 167, loss = 0.97578073\n",
            "Iteration 168, loss = 0.97138163\n",
            "Iteration 169, loss = 0.96703642\n",
            "Iteration 170, loss = 0.96273689\n",
            "Iteration 171, loss = 0.95856581\n",
            "Iteration 172, loss = 0.95442427\n",
            "Iteration 173, loss = 0.95036166\n",
            "Iteration 174, loss = 0.94639185\n",
            "Iteration 175, loss = 0.94245335\n",
            "Iteration 176, loss = 0.93860266\n",
            "Iteration 177, loss = 0.93481677\n",
            "Iteration 178, loss = 0.93107038\n",
            "Iteration 179, loss = 0.92740700\n",
            "Iteration 180, loss = 0.92378947\n",
            "Iteration 181, loss = 0.92021851\n",
            "Iteration 182, loss = 0.91671908\n",
            "Iteration 183, loss = 0.91325424\n",
            "Iteration 184, loss = 0.90983918\n",
            "Iteration 185, loss = 0.90648216\n",
            "Iteration 186, loss = 0.90315415\n",
            "Iteration 187, loss = 0.89987438\n",
            "Iteration 188, loss = 0.89664040\n",
            "Iteration 189, loss = 0.89343288\n",
            "Iteration 190, loss = 0.89026837\n",
            "Iteration 191, loss = 0.88714015\n",
            "Iteration 192, loss = 0.88403642\n",
            "Iteration 193, loss = 0.88096902\n",
            "Iteration 194, loss = 0.87793102\n",
            "Iteration 195, loss = 0.87491489\n",
            "Iteration 196, loss = 0.87192844\n",
            "Iteration 197, loss = 0.86896660\n",
            "Iteration 198, loss = 0.86602361\n",
            "Iteration 199, loss = 0.86310415\n",
            "Iteration 200, loss = 0.86020573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='logistic', hidden_layer_sizes=(3,),\n",
              "              learning_rate_init=0.08, verbose=True)"
            ],
            "text/html": [
              "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(3,),\n",
              "              learning_rate_init=0.08, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;logistic&#x27;, hidden_layer_sizes=(3,),\n",
              "              learning_rate_init=0.08, verbose=True)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlp.score(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "14NgiubpsVXB",
        "outputId": "234e7516-599e-4a7f-981a-a686dd405aa5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.828125"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Depois de treinar dá pra fazer previsoes\n",
        "y_pred = mlp.predict(X_teste)"
      ],
      "metadata": {
        "id": "eT0kLNmWsbwL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Avaliar o modelo"
      ],
      "metadata": {
        "id": "jSldEXi6e85i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report\n"
      ],
      "metadata": {
        "id": "jLG3egytfnkT"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acuracia = accuracy_score(y_teste, y_pred)\n",
        "report = classification_report(y_teste, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6X3zKipfHUM",
        "outputId": "a556ff4d-1562-4996-977b-9bcdbb01cbdc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    }
  ]
}